{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# X_Scraper end-to-end walkthrough\n\n",
        "This notebook is self contained so it runs cleanly in Google Colab without needing a local Python package.\n",
        "It installs the required dependencies, defines the scraper/cleaning/sentiment helpers, and walks through configuration,\n",
        "scraping, cleaning, sentiment analysis, and reporting.\n\n",
        "Use the configuration cell to change the source URL or thresholds, and refer to the interpretation notes at the end\n",
        "for guidance on reading the outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "%pip install -q --disable-pip-version-check pandas requests vaderSentiment matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helpers (defined inline)\n\n",
        "All logic lives in this notebook so you do not need to import `x_scraper`. Feel free to tweak the helper functions\n",
        "here if you want to customize the cleaning rules or sentiment thresholds.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport pandas as pd\nimport requests\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\n\n@dataclass\nclass ScraperConfig:\n    \"\"\"Configuration for fetching, cleaning, and reporting scraped content.\"\"\"\n\n    source_url: str = \"https://jsonplaceholder.typicode.com/posts\"\n    limit: int = 20\n    min_chars: int = 40\n    output_dir: Path = Path(\"artifacts\")\n    detail_filename: str = \"scraped_posts.csv\"\n    report_filename: str = \"sentiment_report.csv\"\n\n    def resolved_output_dir(self) -> Path:\n        \"\"\"Return a fully expanded output directory path.\"\"\"\n\n        return Path(self.output_dir).expanduser().resolve()\n\n\nclass XScraper:\n    \"\"\"Basic scraper and sentiment pipeline.\"\"\"\n\n    def __init__(self, config: ScraperConfig):\n        self.config = config\n        self._analyzer = SentimentIntensityAnalyzer()\n        self._session = requests.Session()\n\n    def scrape(self) -> pd.DataFrame:\n        response = self._session.get(self.config.source_url, timeout=15)\n        response.raise_for_status()\n        payload = response.json()\n\n        normalized: List[Dict[str, str]] = []\n        for raw_post in payload[: self.config.limit]:\n            content = self._merge_fields(raw_post)\n            normalized.append(\n                {\n                    \"id\": raw_post.get(\"id\"),\n                    \"title\": raw_post.get(\"title\", \"\"),\n                    \"body\": raw_post.get(\"body\", \"\"),\n                    \"content\": content,\n                }\n            )\n        return pd.DataFrame(normalized)\n\n    def clean(self, frame: pd.DataFrame) -> pd.DataFrame:\n        clean_frame = frame.copy()\n        clean_frame[\"clean_text\"] = (\n            clean_frame[\"content\"].astype(str)\n            .str.replace(\"\\\\s+\", \" \", regex=True)\n            .str.strip()\n        )\n        if self.config.min_chars:\n            clean_frame = clean_frame[clean_frame[\"clean_text\"].str.len() >= self.config.min_chars]\n        clean_frame.reset_index(drop=True, inplace=True)\n        return clean_frame\n\n    def score_sentiment(self, frame: pd.DataFrame) -> pd.DataFrame:\n        scored = frame.copy()\n\n        def _score_row(text: str) -> Tuple[float, str]:\n            scores = self._analyzer.polarity_scores(text)\n            compound = scores[\"compound\"]\n            if compound >= 0.05:\n                label = \"positive\"\n            elif compound <= -0.05:\n                label = \"negative\"\n            else:\n                label = \"neutral\"\n            return compound, label\n\n        scored[[\"sentiment_score\", \"sentiment\"]] = scored[\"clean_text\"].apply(\n            lambda text: pd.Series(_score_row(text))\n        )\n        return scored\n\n    def generate_reports(self, frame: pd.DataFrame):\n        output_dir = self.config.resolved_output_dir()\n        output_dir.mkdir(parents=True, exist_ok=True)\n\n        detail_path = output_dir / self.config.detail_filename\n        frame.to_csv(detail_path, index=False)\n\n        summary = frame[\"sentiment\"].value_counts().rename_axis(\"sentiment\").reset_index(name=\"count\")\n        summary_path = output_dir / self.config.report_filename\n        summary.to_csv(summary_path, index=False)\n\n        return {\n            \"detail_csv\": detail_path,\n            \"sentiment_report_csv\": summary_path,\n        }\n\n    @staticmethod\n    def _merge_fields(raw_post: Dict[str, str]) -> str:\n        title = raw_post.get(\"title\", \"\")\n        body = raw_post.get(\"body\", \"\")\n        return f\"{title}\\n{body}\".strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n\n",
        "Adjust the values below to control the run:\n\n",
        "- `source_url`: API endpoint or JSON feed to scrape.\n",
        "- `limit`: Maximum number of records to fetch.\n",
        "- `min_chars`: Minimum character length to keep after cleaning (set to 0 to skip filtering).\n",
        "- `output_dir`: Directory where CSV outputs are written.\n",
        "- `detail_filename` / `report_filename`: File names for the detailed rows and aggregated sentiment counts.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "config = ScraperConfig(\n",
        "    source_url=\"https://jsonplaceholder.typicode.com/posts\",  # swap in your endpoint\n",
        "    limit=25,\n",
        "    min_chars=40,\n",
        "    output_dir=Path(\"artifacts\"),\n",
        ")\n",
        "config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scrape\n\n",
        "The cell below downloads posts from the configured `source_url`. For a different dataset, update the configuration above or inject your own DataFrame if you already have raw content.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "pipeline = XScraper(config)\n",
        "raw_posts = pipeline.scrape()\n",
        "raw_posts.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean the text\n\n",
        "The cleaning step collapses whitespace and trims short entries. Increase `min_chars` if you see noisy, low-value rows, or set it to `0` to keep everything.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "clean_posts = pipeline.clean(raw_posts)\n",
        "clean_posts.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sentiment analysis\n\n",
        "VADER sentiment is applied to the cleaned text. The `sentiment_score` is the compound score in `[-1, 1]` and the `sentiment` label is derived from the thresholds used by the analyzer (`>= 0.05` positive, `<= -0.05` negative, otherwise neutral).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "sentiment_posts = pipeline.score_sentiment(clean_posts)\n",
        "sentiment_posts[[\"id\", \"sentiment_score\", \"sentiment\"]].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reporting\n\n",
        "Both detailed and aggregated reports are saved under `output_dir`:\n\n",
        "- `scraped_posts.csv`: Each cleaned post with sentiment columns.\n",
        "- `sentiment_report.csv`: Counts of positive/neutral/negative posts.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "report_paths = pipeline.generate_reports(sentiment_posts)\n",
        "report_paths\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "summary = sentiment_posts[\"sentiment\"].value_counts().rename_axis(\"sentiment\").reset_index(name=\"count\")\n",
        "summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpreting the outputs\n\n",
        "- Review the `summary` table to see whether your dataset skews positive, neutral, or negative.\n",
        "- Inspect the `scraped_posts.csv` file to confirm that cleaning preserved the important text. If it looks truncated, lower `min_chars`.\n",
        "- Check `sentiment_report.csv` for quick counts you can chart elsewhere. You can also use the `summary` DataFrame directly in this notebook to create plots.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}