{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# X.com user scraper, cleaner, and sentiment walkthrough\n",
        "\n",
        "Run this notebook end-to-end in Google Colab without any extra files. It installs dependencies, scrapes recent posts for a list of X.com usernames, cleans and scores sentiment, and saves CSV reports. Every editable setting is surfaced in the configuration cell so you can point the scraper at your own accounts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q --disable-pip-version-check pandas vaderSentiment matplotlib snscrape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helpers (defined inline)\n",
        "\n",
        "All logic lives in this notebook. The scraper uses [`snscrape`](https://github.com/JustAnotherArchivist/snscrape) to pull posts by username without needing an API token. Sentiment is scored with VADER."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ScraperConfig:\n",
        "    \"\"\"Configuration for scraping X.com users and reporting sentiment.\"\"\"\n",
        "\n",
        "    usernames: List[str]\n",
        "    max_posts_per_user: int = 50\n",
        "    min_chars: int = 20\n",
        "    output_dir: Path = Path(\"artifacts\")\n",
        "    detail_filename: str = \"x_posts.csv\"\n",
        "    report_filename: str = \"sentiment_report.csv\"\n",
        "\n",
        "    def resolved_output_dir(self) -> Path:\n",
        "        return Path(self.output_dir).expanduser().resolve()\n",
        "\n",
        "\n",
        "class XUserScraper:\n",
        "    \"\"\"Scrape posts for one or more X.com usernames.\"\"\"\n",
        "\n",
        "    def __init__(self, config: ScraperConfig):\n",
        "        self.config = config\n",
        "        self._analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "    def scrape(self) -> pd.DataFrame:\n",
        "        rows = []\n",
        "        for username in self.config.usernames:\n",
        "            scraper = sntwitter.TwitterUserScraper(username=username)\n",
        "            for idx, tweet in enumerate(scraper.get_items()):\n",
        "                if idx >= self.config.max_posts_per_user:\n",
        "                    break\n",
        "                rows.append(\n",
        "                    {\n",
        "                        \"username\": username,\n",
        "                        \"id\": tweet.id,\n",
        "                        \"date\": tweet.date.isoformat(),\n",
        "                        \"content\": tweet.content,\n",
        "                        \"language\": getattr(tweet, \"lang\", None),\n",
        "                        \"likeCount\": tweet.likeCount,\n",
        "                        \"replyCount\": tweet.replyCount,\n",
        "                        \"retweetCount\": tweet.retweetCount,\n",
        "                        \"quoteCount\": tweet.quoteCount,\n",
        "                        \"viewCount\": getattr(tweet, \"viewCount\", None),\n",
        "                        \"url\": tweet.url,\n",
        "                    }\n",
        "                )\n",
        "        return pd.DataFrame(rows)\n",
        "\n",
        "    def clean(self, frame: pd.DataFrame) -> pd.DataFrame:\n",
        "        clean_frame = frame.copy()\n",
        "        clean_frame[\"clean_text\"] = (\n",
        "            clean_frame[\"content\"].astype(str)\n",
        "            .str.replace(r\"\\s+\", \" \", regex=True)\n",
        "            .str.strip()\n",
        "        )\n",
        "        if self.config.min_chars:\n",
        "            clean_frame = clean_frame[clean_frame[\"clean_text\"].str.len() >= self.config.min_chars]\n",
        "        clean_frame.reset_index(drop=True, inplace=True)\n",
        "        return clean_frame\n",
        "\n",
        "    def score_sentiment(self, frame: pd.DataFrame) -> pd.DataFrame:\n",
        "        scored = frame.copy()\n",
        "\n",
        "        def _score_row(text: str) -> Tuple[float, str]:\n",
        "            scores = self._analyzer.polarity_scores(text)\n",
        "            compound = scores[\"compound\"]\n",
        "            if compound >= 0.05:\n",
        "                label = \"positive\"\n",
        "            elif compound <= -0.05:\n",
        "                label = \"negative\"\n",
        "            else:\n",
        "                label = \"neutral\"\n",
        "            return compound, label\n",
        "\n",
        "        scored[[\"sentiment_score\", \"sentiment\"]] = scored[\"clean_text\"].apply(\n",
        "            lambda text: pd.Series(_score_row(text))\n",
        "        )\n",
        "        return scored\n",
        "\n",
        "    def generate_reports(self, frame: pd.DataFrame):\n",
        "        output_dir = self.config.resolved_output_dir()\n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        detail_path = output_dir / self.config.detail_filename\n",
        "        frame.to_csv(detail_path, index=False)\n",
        "\n",
        "        summary = frame[\"sentiment\"].value_counts().rename_axis(\"sentiment\").reset_index(name=\"count\")\n",
        "        summary_path = output_dir / self.config.report_filename\n",
        "        summary.to_csv(summary_path, index=False)\n",
        "\n",
        "        return {\n",
        "            \"detail_csv\": detail_path,\n",
        "            \"sentiment_report_csv\": summary_path,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Edit the usernames list to target the accounts you want to scrape (10\u201315 usernames work fine).\n",
        "\n",
        "- `usernames`: X.com handles without the `@`.\n",
        "- `max_posts_per_user`: How many recent posts to pull for each username.\n",
        "- `min_chars`: Minimum character length to keep after cleaning (set to `0` to keep everything).\n",
        "- `output_dir` / filenames: Where CSV exports are written."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "usernames = [\n",
        "    \"X\",  # replace with your target handles (no @)\n",
        "    \"TwitterDev\",\n",
        "    \"jack\",\n",
        "]\n",
        "\n",
        "config = ScraperConfig(\n",
        "    usernames=usernames,\n",
        "    max_posts_per_user=40,\n",
        "    min_chars=20,\n",
        "    output_dir=Path(\"artifacts\"),\n",
        ")\n",
        "config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scrape\n",
        "\n",
        "Run the cell below to fetch posts for the configured usernames. The resulting DataFrame includes the posting date and common engagement fields so you can inspect what is available before cleaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline = XUserScraper(config)\n",
        "raw_posts = pipeline.scrape()\n",
        "raw_posts.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspect the available columns. Feel free to slice the DataFrame however you like; the `date`, `content`, and engagement counts are already included."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_posts[[\"username\", \"date\", \"content\", \"likeCount\", \"replyCount\", \"retweetCount\", \"quoteCount\"]].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean the text\n",
        "\n",
        "Whitespace is collapsed and rows shorter than `min_chars` are dropped. Adjust `min_chars` in the config if you want to keep shorter posts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clean_posts = pipeline.clean(raw_posts)\n",
        "clean_posts.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sentiment analysis\n",
        "\n",
        "VADER sentiment is applied to the cleaned text. `sentiment_score` is the compound score in `[-1, 1]`; `sentiment` is the bucketed label (positive/neutral/negative)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentiment_posts = pipeline.score_sentiment(clean_posts)\n",
        "sentiment_posts[[\"username\", \"date\", \"sentiment_score\", \"sentiment\"]].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reporting\n",
        "\n",
        "Detailed and aggregated CSVs are saved under `output_dir` for reuse outside the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "report_paths = pipeline.generate_reports(sentiment_posts)\n",
        "report_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary = sentiment_posts[\"sentiment\"].value_counts().rename_axis(\"sentiment\").reset_index(name=\"count\")\n",
        "summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpreting the outputs\n",
        "\n",
        "- Use `summary` to understand whether the scraped accounts skew positive, neutral, or negative.\n",
        "- Open `x_posts.csv` to audit the cleaned text and ensure important content was preserved. Lower `min_chars` if rows look truncated.\n",
        "- `sentiment_report.csv` is a quick export of label counts you can plot elsewhere."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}